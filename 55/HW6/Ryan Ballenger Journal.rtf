{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 CourierNewPS-BoldMT;\f2\froman\fcharset0 Times-Roman;
\f3\fnil\fcharset0 Menlo-Regular;\f4\fmodern\fcharset0 Courier-Bold;\f5\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red232\green87\blue234;\red186\green140\blue28;}
\margl1440\margr1440\vieww18200\viewh14520\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \
Ryan Ballenger\
Step-by-step Journal \
\
1. Downloaded hadoop tarball.\
\
2. In terminal, went to /usr/share and called \'93
\f1\b\fs28 $ sudo chown $USER .\'94
\f0\b0\fs24  to become the owner
\f1\b\fs28  \

\f0\b0\fs24 	- entered my password as requested\
\
3. Ran \'93
\f1\b\fs28 $ tar xfz hadoop-2.1.7.tar.gz\'94 
\f0\b0\fs24 within /usr/share
\f1\b\fs28 \

\f0\b0\fs24 	- no output\
\
4. Added Hadoop utilities to my execution path \
	
\f1\b\fs28 $ export HADOOP_HOME=/usr/share/hadoop-2.7.1
\f2\b0 \

\f0\fs24 		- no output\
	
\f1\b\fs28 $ export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
\f2\b0 \

\f0\fs24 		- no output \
\
5. Used emacs to open ~/.profile, since ~/.bash and ~/.bash_profile didn\'92t exist on my 2009 MacBook Pro \
	Added the following lines:\
	
\f3\fs22 \cf2 \CocoaLigature0 export\cf0  \cf3 HADOOP_HOME\cf0 =/usr/share/hadoop-2.7.1\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf2 	 export\cf0  \cf3 PATH\cf0 =$\cf3 PATH\cf0 :$\cf3 HADOOP_HOME\cf0 /bin:$\cf3 HADOOP_HOME\cf0 /sbin\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \CocoaLigature1 5. Create environment variable JAVA_HOME\
	Called 
\f4\b /usr/libexec/java_home
\f5\b0 \

\f0 	to find 
\f3\fs22 \CocoaLigature0 /Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home
\f0\fs24 \CocoaLigature1 \
	
\f1\b $ export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home
\f5\b0 \

\f0 \
6. Verify the install \
	
\f4\b $ hadoop version
\f5\b0 \

\f0 	output:
\f3\fs22 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 		Hadoop 2.7.1\
		Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a\
		Compiled by jenkins on 2015-06-29T06:04Z\
		Compiled with protoc 2.5.0\
		From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a\
		This command was run using /usr/share/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
7. Copied config files to a safe location\
	
\f4\b $ cd $HADOOP_HOME/etc\
\pard\pardeftab720
\cf0  	$ cp -r hadoop hadoop-deployed\
	$ export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop-deployed
\f0\b0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 	Also added the last line to ~/.profile \
\
8. Edited 
\f1\b\fs28 $HADOOP_HOME/etc/hadoop-deployed/core-site.xml 
\f0\b0\fs24 to\
	
\f4\b <configuration>\
\pard\pardeftab720
\cf0   		<property>\
    			<name>fs.defaultFS</name>\
    			<value>hdfs://localhost/</value>\
  		</property>\
	</configuration>
\f5\b0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0 \cf0 \
9. Edited 
\f1\b\fs28 $HADOOP_HOME/etc/hadoop-deployed/hdfs-site.xml 
\f0\b0\fs24 to\
	
\f4\b\fs28 <configuration>\
\pard\pardeftab720
\cf0   		<property>\
    			<name>dfs.replication</name>\
    			<value>1</value>\
  		</property>\
  <!-- Arrange the Hadoop backing store for name and data -->\
  <!-- Use your home directory where you see /Users/charliesawyer -->\
  		<property>\
    			<name>dfs.name.dir</name>\
    			<value>/Users/charliesawyer/hadoop/hadoop-name</value>\
  		</property>\
  		<property>\
    			<name>dfs.data.dir</name>\
    			<value>/Users/charliesawyer/hadoop/hadoop-data</value>\
  		</property>\
	</configuration>\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\b0\fs24 \cf0 \
10. Enabled secure shell no-password access.\
	
\f1\b\fs28 $ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
\f2\b0 \

\f0\fs24 	
\f1\b\fs28 $cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
\f2\b0 \

\f0\fs24 	Next, verified ssh running\
	
\f1\b\fs28 $ssh localhost\

\f0\b0\fs24 	Output: 
\f3\fs22 \CocoaLigature0 Last login: Wed Dec  2 20:06:37 2015
\f0\fs24 \CocoaLigature1 \
	\
11. Created soft link to /bin/java\
	Located into /bin/ \
	Called  
\f1\b\fs28 $ sudo ln -s /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java java\
	
\f0\b0\fs24 Output: none\
\
12. Initialized the HDFS filesystem \
	Called: 
\f1\b\fs28 $ hdfs namenode -format
\f0\b0\fs24  \
	Output: 
\f3\fs22 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 15/11/30 22:49:54 INFO namenode.NameNode: STARTUP_MSG: \
/************************************************************\
STARTUP_MSG: Starting NameNode\
STARTUP_MSG:   host = Ryans-MacBook-Pro.local/10.251.240.245\
STARTUP_MSG:   args = [-format]\
STARTUP_MSG:   version = 2.7.1\
STARTUP_MSG:   classpath = /usr/share/hadoop-2.7.1/etc/hadoop-deployed:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/share/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/share/hadoop-2.7.1/contrib/capacity-scheduler/*.jar\
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\
STARTUP_MSG:   java = 1.8.0_51\
************************************************************/\
15/11/30 22:49:54 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\
15/11/30 22:49:54 INFO namenode.NameNode: createNameNode [-format]\
15/11/30 22:49:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
15/11/30 22:49:56 WARN common.Util: Path /Users/Ryan/hadoop/hadoop-name should be specified as a URI in configuration files. Please update hdfs configuration.\
15/11/30 22:49:56 WARN common.Util: Path /Users/Ryan/hadoop/hadoop-name should be specified as a URI in configuration files. Please update hdfs configuration.\
Formatting using clusterid: CID-a46d0f7b-6e0e-4b97-b5d0-543468845234\
15/11/30 22:49:57 INFO namenode.FSNamesystem: No KeyProvider found.\
15/11/30 22:49:57 INFO namenode.FSNamesystem: fsLock is fair:true\
15/11/30 22:49:57 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\
15/11/30 22:49:57 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: The block deletion will start around 2015 Nov 30 22:49:57\
15/11/30 22:49:57 INFO util.GSet: Computing capacity for map BlocksMap\
15/11/30 22:49:57 INFO util.GSet: VM type       = 64-bit\
15/11/30 22:49:57 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\
15/11/30 22:49:57 INFO util.GSet: capacity      = 2^21 = 2097152 entries\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: defaultReplication         = 1\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: maxReplication             = 512\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: minReplication             = 1\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\
15/11/30 22:49:57 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\
15/11/30 22:49:57 INFO namenode.FSNamesystem: fsOwner             = Ryan (auth:SIMPLE)\
15/11/30 22:49:57 INFO namenode.FSNamesystem: supergroup          = supergroup\
15/11/30 22:49:57 INFO namenode.FSNamesystem: isPermissionEnabled = true\
15/11/30 22:49:57 INFO namenode.FSNamesystem: HA Enabled: false\
15/11/30 22:49:57 INFO namenode.FSNamesystem: Append Enabled: true\
15/11/30 22:49:58 INFO util.GSet: Computing capacity for map INodeMap\
15/11/30 22:49:58 INFO util.GSet: VM type       = 64-bit\
15/11/30 22:49:58 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\
15/11/30 22:49:58 INFO util.GSet: capacity      = 2^20 = 1048576 entries\
15/11/30 22:49:58 INFO namenode.FSDirectory: ACLs enabled? false\
15/11/30 22:49:58 INFO namenode.FSDirectory: XAttrs enabled? true\
15/11/30 22:49:58 INFO namenode.FSDirectory: Maximum size of an xattr: 16384\
15/11/30 22:49:58 INFO namenode.NameNode: Caching file names occuring more than 10 times\
15/11/30 22:49:58 INFO util.GSet: Computing capacity for map cachedBlocks\
15/11/30 22:49:58 INFO util.GSet: VM type       = 64-bit\
15/11/30 22:49:58 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\
15/11/30 22:49:58 INFO util.GSet: capacity      = 2^18 = 262144 entries\
15/11/30 22:49:58 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\
15/11/30 22:49:58 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\
15/11/30 22:49:58 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\
15/11/30 22:49:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\
15/11/30 22:49:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\
15/11/30 22:49:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\
15/11/30 22:49:58 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\
15/11/30 22:49:58 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\
15/11/30 22:49:59 INFO util.GSet: Computing capacity for map NameNodeRetryCache\
15/11/30 22:49:59 INFO util.GSet: VM type       = 64-bit\
15/11/30 22:49:59 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\
15/11/30 22:49:59 INFO util.GSet: capacity      = 2^15 = 32768 entries\
15/11/30 22:49:59 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1408085194-10.251.240.245-1448941799106\
15/11/30 22:49:59 INFO common.Storage: Storage directory /Users/Ryan/hadoop/hadoop-name has been successfully formatted.\
15/11/30 22:49:59 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\
15/11/30 22:49:59 INFO util.ExitUtil: Exiting with status 0\
15/11/30 22:49:59 INFO namenode.NameNode: SHUTDOWN_MSG: \
/************************************************************\
SHUTDOWN_MSG: Shutting down NameNode at Ryans-MacBook-Pro.local/10.251.240.245\
************************************************************/
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
13. Start single-node Hadoop cluster\
	Called: 
\f1\b\fs28 start-dfs.sh\
	
\f0\b0\fs24 Output:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f3\fs22 \cf0 \CocoaLigature0 15/12/02 22:24:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
Starting namenodes on [localhost]\
localhost: starting namenode, logging to /usr/share/hadoop-2.7.1/logs/hadoop-Ryan-namenode-Ryans-MacBook-Pro.local.out\
localhost: starting datanode, logging to /usr/share/hadoop-2.7.1/logs/hadoop-Ryan-datanode-Ryans-MacBook-Pro.local.out\
Starting secondary namenodes [0.0.0.0]\
0.0.0.0: starting secondarynamenode, logging to /usr/share/hadoop-2.7.1/logs/hadoop-Ryan-secondarynamenode-Ryans-MacBook-Pro.local.out\
15/12/02 22:25:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \CocoaLigature1 14. Confirmed successful startup \
	Called: 
\f1\b\fs28 jps\

\f0\b0\fs24 	Output:
\f3\fs22 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 2448 SecondaryNameNode\
2245 NameNode\
2522 Jps\
2335 DataNode
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\
15. Downloaded and unzip hw6.zip. Also moved fleas or jabberwocky files into input folder. Compiled WordCount.java\
	\
	From within the hw6 folder called:
\f3\fs22 \CocoaLigature0 javac -cp $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.7.1.jar:$HADOOP_HOME/share/hadoop/tools/lib/commons-cli-1.2.jar WordCount.java\
\
	
\f0\fs24 \CocoaLigature1 Output:
\f3\fs22 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 /usr/share/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar(org/apache/hadoop/fs/Path.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate': class file for org.apache.hadoop.classification.InterfaceAudience not found\
Note: WordCount.java uses or overrides a deprecated API.\
Note: Recompile with -Xlint:deprecation for details.\
Note: WordCount.java uses unchecked or unsafe operations.\
Note: Recompile with -Xlint:unchecked for details.\
1 warning\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \CocoaLigature1 \
\
\
16. Set HADOOP_CLASSPATH\
	\
	Called: 
\f3\fs22 \CocoaLigature0 export HADOOP_CLASSPATH=$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.7.1.jar:$HADOOP_HOME/share/hadoop/tools/lib/commons-cli-1.2.jar:.\
	
\f0\fs24 \CocoaLigature1 \
	Output: none\
\
\
17. Run WordCount locally \
\
From within hw6 folder called:  
\f4\b hadoop WordCount -fs file:/// -jt local input output
\f5\b0 \

\f0 Output for Jabberwocky:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f3\fs22 \cf0 \CocoaLigature0 15/12/02 23:16:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
15/12/02 23:16:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\
15/12/02 23:16:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\
15/12/02 23:16:09 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\
15/12/02 23:16:09 INFO input.FileInputFormat: Total input paths to process : 1\
15/12/02 23:16:09 INFO mapreduce.JobSubmitter: number of splits:1\
15/12/02 23:16:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1608209199_0001\
15/12/02 23:16:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\
15/12/02 23:16:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:16:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\
15/12/02 23:16:09 INFO mapred.LocalJobRunner: Waiting for map tasks\
15/12/02 23:16:09 INFO mapred.LocalJobRunner: Starting task: attempt_local1608209199_0001_m_000000_0\
15/12/02 23:16:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:16:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:16:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:16:09 INFO mapred.MapTask: Processing split: file:/Users/Ryan/Desktop/hw6/input/jabberwocky:0+1032\
15/12/02 23:16:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\
15/12/02 23:16:09 INFO mapreduce.Job: Running job: job_local1608209199_0001\
15/12/02 23:16:09 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\
15/12/02 23:16:09 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\
15/12/02 23:16:09 INFO mapred.MapTask: soft limit at 83886080\
15/12/02 23:16:09 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\
15/12/02 23:16:09 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\
15/12/02 23:16:09 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\
15/12/02 23:16:09 INFO mapred.LocalJobRunner: \
15/12/02 23:16:09 INFO mapred.MapTask: Starting flush of map output\
15/12/02 23:16:09 INFO mapred.MapTask: Spilling map output\
15/12/02 23:16:09 INFO mapred.MapTask: bufstart = 0; bufend = 1606; bufvoid = 104857600\
15/12/02 23:16:09 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213736(104854944); length = 661/6553600\
15/12/02 23:16:09 INFO mapred.MapTask: Finished spill 0\
15/12/02 23:16:10 INFO mapred.Task: Task:attempt_local1608209199_0001_m_000000_0 is done. And is in the process of committing\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: map\
15/12/02 23:16:10 INFO mapred.Task: Task 'attempt_local1608209199_0001_m_000000_0' done.\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local1608209199_0001_m_000000_0\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: map task executor complete.\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: Waiting for reduce tasks\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: Starting task: attempt_local1608209199_0001_r_000000_0\
15/12/02 23:16:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:16:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:16:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:16:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@63f578fd\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\
15/12/02 23:16:10 INFO reduce.EventFetcher: attempt_local1608209199_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\
15/12/02 23:16:10 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1608209199_0001_m_000000_0 decomp: 1231 len: 1235 to MEMORY\
15/12/02 23:16:10 INFO reduce.InMemoryMapOutput: Read 1231 bytes from map-output for attempt_local1608209199_0001_m_000000_0\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1231, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1231\
15/12/02 23:16:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\
15/12/02 23:16:10 INFO mapred.Merger: Merging 1 sorted segments\
15/12/02 23:16:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1225 bytes\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 1231 bytes to disk to satisfy reduce memory limit\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: Merging 1 files, 1235 bytes from disk\
15/12/02 23:16:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\
15/12/02 23:16:10 INFO mapred.Merger: Merging 1 sorted segments\
15/12/02 23:16:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1225 bytes\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:16:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\
15/12/02 23:16:10 INFO mapred.Task: Task:attempt_local1608209199_0001_r_000000_0 is done. And is in the process of committing\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:16:10 INFO mapred.Task: Task attempt_local1608209199_0001_r_000000_0 is allowed to commit now\
15/12/02 23:16:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1608209199_0001_r_000000_0' to file:/Users/Ryan/Desktop/hw6/output/_temporary/0/task_local1608209199_0001_r_000000\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: reduce > reduce\
15/12/02 23:16:10 INFO mapred.Task: Task 'attempt_local1608209199_0001_r_000000_0' done.\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local1608209199_0001_r_000000_0\
15/12/02 23:16:10 INFO mapred.LocalJobRunner: reduce task executor complete.\
15/12/02 23:16:10 INFO mapreduce.Job: Job job_local1608209199_0001 running in uber mode : false\
15/12/02 23:16:10 INFO mapreduce.Job:  map 100% reduce 100%\
15/12/02 23:16:10 INFO mapreduce.Job: Job job_local1608209199_0001 completed successfully\
15/12/02 23:16:10 INFO mapreduce.Job: Counters: 30\
	File System Counters\
		FILE: Number of bytes read=4900\
		FILE: Number of bytes written=553369\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
	Map-Reduce Framework\
		Map input records=34\
		Map output records=166\
		Map output bytes=1606\
		Map output materialized bytes=1235\
		Input split bytes=111\
		Combine input records=166\
		Combine output records=100\
		Reduce input groups=100\
		Reduce shuffle bytes=1235\
		Reduce input records=100\
		Reduce output records=100\
		Spilled Records=200\
		Shuffled Maps =1\
		Failed Shuffles=0\
		Merged Map outputs=1\
		GC time elapsed (ms)=0\
		Total committed heap usage (bytes)=440401920\
	Shuffle Errors\
		BAD_ID=0\
		CONNECTION=0\
		IO_ERROR=0\
		WRONG_LENGTH=0\
		WRONG_MAP=0\
		WRONG_REDUCE=0\
	File Input Format Counters \
		Bytes Read=1032\
	File Output Format Counters \
		Bytes Written=846
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
Output for Fleas:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f3\fs22 \cf0 \CocoaLigature0 15/12/02 23:11:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
15/12/02 23:11:17 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\
15/12/02 23:11:17 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\
15/12/02 23:11:17 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\
15/12/02 23:11:17 INFO input.FileInputFormat: Total input paths to process : 2\
15/12/02 23:11:17 INFO mapreduce.JobSubmitter: number of splits:2\
15/12/02 23:11:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local984145845_0001\
15/12/02 23:11:17 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\
15/12/02 23:11:17 INFO mapreduce.Job: Running job: job_local984145845_0001\
15/12/02 23:11:17 INFO mapred.LocalJobRunner: OutputCommitter set in config null\
15/12/02 23:11:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:11:17 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Waiting for map tasks\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Starting task: attempt_local984145845_0001_m_000000_0\
15/12/02 23:11:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:11:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:11:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:11:18 INFO mapred.MapTask: Processing split: file:/Users/Ryan/Desktop/hw6/input/fleas1.txt:0+60\
15/12/02 23:11:18 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\
15/12/02 23:11:18 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\
15/12/02 23:11:18 INFO mapred.MapTask: soft limit at 83886080\
15/12/02 23:11:18 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\
15/12/02 23:11:18 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\
15/12/02 23:11:18 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: \
15/12/02 23:11:18 INFO mapred.MapTask: Starting flush of map output\
15/12/02 23:11:18 INFO mapred.MapTask: Spilling map output\
15/12/02 23:11:18 INFO mapred.MapTask: bufstart = 0; bufend = 104; bufvoid = 104857600\
15/12/02 23:11:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600\
15/12/02 23:11:18 INFO mapred.MapTask: Finished spill 0\
15/12/02 23:11:18 INFO mapred.Task: Task:attempt_local984145845_0001_m_000000_0 is done. And is in the process of committing\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: map\
15/12/02 23:11:18 INFO mapred.Task: Task 'attempt_local984145845_0001_m_000000_0' done.\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local984145845_0001_m_000000_0\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Starting task: attempt_local984145845_0001_m_000001_0\
15/12/02 23:11:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:11:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:11:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:11:18 INFO mapred.MapTask: Processing split: file:/Users/Ryan/Desktop/hw6/input/fleas2.txt:0+56\
15/12/02 23:11:18 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\
15/12/02 23:11:18 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\
15/12/02 23:11:18 INFO mapred.MapTask: soft limit at 83886080\
15/12/02 23:11:18 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\
15/12/02 23:11:18 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\
15/12/02 23:11:18 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: \
15/12/02 23:11:18 INFO mapred.MapTask: Starting flush of map output\
15/12/02 23:11:18 INFO mapred.MapTask: Spilling map output\
15/12/02 23:11:18 INFO mapred.MapTask: bufstart = 0; bufend = 100; bufvoid = 104857600\
15/12/02 23:11:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600\
15/12/02 23:11:18 INFO mapred.MapTask: Finished spill 0\
15/12/02 23:11:18 INFO mapred.Task: Task:attempt_local984145845_0001_m_000001_0 is done. And is in the process of committing\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: map\
15/12/02 23:11:18 INFO mapred.Task: Task 'attempt_local984145845_0001_m_000001_0' done.\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local984145845_0001_m_000001_0\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: map task executor complete.\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Waiting for reduce tasks\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Starting task: attempt_local984145845_0001_r_000000_0\
15/12/02 23:11:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:11:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:11:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:11:18 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ac3b638\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\
15/12/02 23:11:18 INFO reduce.EventFetcher: attempt_local984145845_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\
15/12/02 23:11:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local984145845_0001_m_000001_0 decomp: 124 len: 128 to MEMORY\
15/12/02 23:11:18 INFO reduce.InMemoryMapOutput: Read 124 bytes from map-output for attempt_local984145845_0001_m_000001_0\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 124, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->124\
15/12/02 23:11:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local984145845_0001_m_000000_0 decomp: 116 len: 120 to MEMORY\
15/12/02 23:11:18 INFO reduce.InMemoryMapOutput: Read 116 bytes from map-output for attempt_local984145845_0001_m_000000_0\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 116, inMemoryMapOutputs.size() -> 2, commitMemory -> 124, usedMemory ->240\
15/12/02 23:11:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: 2 / 2 copied.\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs\
15/12/02 23:11:18 INFO mapred.Merger: Merging 2 sorted segments\
15/12/02 23:11:18 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 226 bytes\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: Merged 2 segments, 240 bytes to disk to satisfy reduce memory limit\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: Merging 1 files, 242 bytes from disk\
15/12/02 23:11:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\
15/12/02 23:11:18 INFO mapred.Merger: Merging 1 sorted segments\
15/12/02 23:11:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 232 bytes\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: 2 / 2 copied.\
15/12/02 23:11:18 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\
15/12/02 23:11:18 INFO mapred.Task: Task:attempt_local984145845_0001_r_000000_0 is done. And is in the process of committing\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: 2 / 2 copied.\
15/12/02 23:11:18 INFO mapred.Task: Task attempt_local984145845_0001_r_000000_0 is allowed to commit now\
15/12/02 23:11:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local984145845_0001_r_000000_0' to file:/Users/Ryan/Desktop/hw6/output/_temporary/0/task_local984145845_0001_r_000000\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: reduce > reduce\
15/12/02 23:11:18 INFO mapred.Task: Task 'attempt_local984145845_0001_r_000000_0' done.\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local984145845_0001_r_000000_0\
15/12/02 23:11:18 INFO mapred.LocalJobRunner: reduce task executor complete.\
15/12/02 23:11:18 INFO mapreduce.Job: Job job_local984145845_0001 running in uber mode : false\
15/12/02 23:11:18 INFO mapreduce.Job:  map 100% reduce 100%\
15/12/02 23:11:18 INFO mapreduce.Job: Job job_local984145845_0001 completed successfully\
15/12/02 23:11:18 INFO mapreduce.Job: Counters: 30\
	File System Counters\
		FILE: Number of bytes read=2185\
		FILE: Number of bytes written=820203\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
	Map-Reduce Framework\
		Map input records=2\
		Map output records=22\
		Map output bytes=204\
		Map output materialized bytes=248\
		Input split bytes=220\
		Combine input records=22\
		Combine output records=21\
		Reduce input groups=18\
		Reduce shuffle bytes=248\
		Reduce input records=21\
		Reduce output records=18\
		Spilled Records=42\
		Shuffled Maps =2\
		Failed Shuffles=0\
		Merged Map outputs=2\
		GC time elapsed (ms)=0\
		Total committed heap usage (bytes)=871366656\
	Shuffle Errors\
		BAD_ID=0\
		CONNECTION=0\
		IO_ERROR=0\
		WRONG_LENGTH=0\
		WRONG_MAP=0\
		WRONG_REDUCE=0\
	File Input Format Counters \
		Bytes Read=116\
	File Output Format Counters \
		Bytes Written=140
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
18. Running on HDFS via a single-node cluster\
	18.1 Make input folder\
	Command: 
\f3\fs22 \CocoaLigature0 $ hadoop fs -mkdir -p /user/$USER/input
\f0\fs24 \CocoaLigature1 \
	Output: 
\f3\fs22 \CocoaLigature0 15/12/02 23:21:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\

\f0\fs24 \CocoaLigature1 \
	18.2 Copy jabberwocky file to input folder\
	Command: 
\f3\fs22 \CocoaLigature0 $ hadoop fs -copyFromLocal jabberwocky input\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 	 
\f0\fs24 \CocoaLigature1 Output: 
\f3\fs22 \CocoaLigature0 15/12/02 23:22:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \CocoaLigature1 \
	18.3 Verify input is in the correct folder\
	Command:
\f3\fs22 \CocoaLigature0  $ hadoop fs -ls input\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 	 
\f0\fs24 \CocoaLigature1 Output: 
\f3\fs22 \CocoaLigature0 15/12/02 23:22:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
		Found 1 items\
		-rw-r--r--   1 Ryan supergroup       1032 2015-12-02 23:22 input/jabberwocky
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
	18.4 Run WordCount on the cluster\
	Command: 
\f3\fs22 \CocoaLigature0 $ hadoop WordCount input output\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 	 
\f0\fs24 \CocoaLigature1 Output: 
\f3\fs22 \CocoaLigature0 15/12/02 23:24:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
15/12/02 23:24:25 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\
15/12/02 23:24:25 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\
15/12/02 23:24:26 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\
15/12/02 23:24:26 INFO input.FileInputFormat: Total input paths to process : 1\
15/12/02 23:24:26 INFO mapreduce.JobSubmitter: number of splits:1\
15/12/02 23:24:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local495726991_0001\
15/12/02 23:24:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\
15/12/02 23:24:27 INFO mapreduce.Job: Running job: job_local495726991_0001\
15/12/02 23:24:27 INFO mapred.LocalJobRunner: OutputCommitter set in config null\
15/12/02 23:24:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:24:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\
15/12/02 23:24:27 INFO mapred.LocalJobRunner: Waiting for map tasks\
15/12/02 23:24:27 INFO mapred.LocalJobRunner: Starting task: attempt_local495726991_0001_m_000000_0\
15/12/02 23:24:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:24:27 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:24:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:24:27 INFO mapred.MapTask: Processing split: hdfs://localhost/user/Ryan/input/jabberwocky:0+1032\
15/12/02 23:24:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\
15/12/02 23:24:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\
15/12/02 23:24:27 INFO mapred.MapTask: soft limit at 83886080\
15/12/02 23:24:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\
15/12/02 23:24:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\
15/12/02 23:24:27 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\
15/12/02 23:24:27 INFO mapred.LocalJobRunner: \
15/12/02 23:24:28 INFO mapred.MapTask: Starting flush of map output\
15/12/02 23:24:28 INFO mapred.MapTask: Spilling map output\
15/12/02 23:24:28 INFO mapred.MapTask: bufstart = 0; bufend = 1606; bufvoid = 104857600\
15/12/02 23:24:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213736(104854944); length = 661/6553600\
15/12/02 23:24:28 INFO mapred.MapTask: Finished spill 0\
15/12/02 23:24:28 INFO mapred.Task: Task:attempt_local495726991_0001_m_000000_0 is done. And is in the process of committing\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: map\
15/12/02 23:24:28 INFO mapred.Task: Task 'attempt_local495726991_0001_m_000000_0' done.\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local495726991_0001_m_000000_0\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: map task executor complete.\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: Waiting for reduce tasks\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: Starting task: attempt_local495726991_0001_r_000000_0\
15/12/02 23:24:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\
15/12/02 23:24:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\
15/12/02 23:24:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\
15/12/02 23:24:28 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@554d2528\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\
15/12/02 23:24:28 INFO reduce.EventFetcher: attempt_local495726991_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\
15/12/02 23:24:28 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local495726991_0001_m_000000_0 decomp: 1231 len: 1235 to MEMORY\
15/12/02 23:24:28 INFO reduce.InMemoryMapOutput: Read 1231 bytes from map-output for attempt_local495726991_0001_m_000000_0\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1231, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1231\
15/12/02 23:24:28 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\
15/12/02 23:24:28 INFO mapred.Merger: Merging 1 sorted segments\
15/12/02 23:24:28 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1225 bytes\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: Merged 1 segments, 1231 bytes to disk to satisfy reduce memory limit\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: Merging 1 files, 1235 bytes from disk\
15/12/02 23:24:28 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\
15/12/02 23:24:28 INFO mapred.Merger: Merging 1 sorted segments\
15/12/02 23:24:28 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1225 bytes\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:24:28 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\
15/12/02 23:24:28 INFO mapreduce.Job: Job job_local495726991_0001 running in uber mode : false\
15/12/02 23:24:28 INFO mapreduce.Job:  map 100% reduce 0%\
15/12/02 23:24:28 INFO mapred.Task: Task:attempt_local495726991_0001_r_000000_0 is done. And is in the process of committing\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\
15/12/02 23:24:28 INFO mapred.Task: Task attempt_local495726991_0001_r_000000_0 is allowed to commit now\
15/12/02 23:24:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_local495726991_0001_r_000000_0' to hdfs://localhost/user/Ryan/output/_temporary/0/task_local495726991_0001_r_000000\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: reduce > reduce\
15/12/02 23:24:28 INFO mapred.Task: Task 'attempt_local495726991_0001_r_000000_0' done.\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local495726991_0001_r_000000_0\
15/12/02 23:24:28 INFO mapred.LocalJobRunner: reduce task executor complete.\
15/12/02 23:24:29 INFO mapreduce.Job:  map 100% reduce 100%\
15/12/02 23:24:29 INFO mapreduce.Job: Job job_local495726991_0001 completed successfully\
15/12/02 23:24:29 INFO mapreduce.Job: Counters: 35\
	File System Counters\
		FILE: Number of bytes read=2840\
		FILE: Number of bytes written=550651\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
		HDFS: Number of bytes read=2064\
		HDFS: Number of bytes written=830\
		HDFS: Number of read operations=13\
		HDFS: Number of large read operations=0\
		HDFS: Number of write operations=4\
	Map-Reduce Framework\
		Map input records=34\
		Map output records=166\
		Map output bytes=1606\
		Map output materialized bytes=1235\
		Input split bytes=109\
		Combine input records=166\
		Combine output records=100\
		Reduce input groups=100\
		Reduce shuffle bytes=1235\
		Reduce input records=100\
		Reduce output records=100\
		Spilled Records=200\
		Shuffled Maps =1\
		Failed Shuffles=0\
		Merged Map outputs=1\
		GC time elapsed (ms)=0\
		Total committed heap usage (bytes)=564133888\
	Shuffle Errors\
		BAD_ID=0\
		CONNECTION=0\
		IO_ERROR=0\
		WRONG_LENGTH=0\
		WRONG_MAP=0\
		WRONG_REDUCE=0\
	File Input Format Counters \
		Bytes Read=1032\
	File Output Format Counters \
		Bytes Written=830
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
19. Show output from running on cluster\
Command: 
\f3\fs22 \CocoaLigature0 $ hadoop fs -cat output/part-r-00000 
\f0\fs24 \CocoaLigature1 	\
Output:
\f3\fs22 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural
\cf0 15/12/02 23:35:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
All	2\
And	5\
And,	1\
Bandersnatch!\'94	1\
Beware	1\
Callay!\'94	1\
Callooh!	1\
Came	1\
Come	1\
Did	2\
He	4\
Jabberwock,	2\
Jabberwock?	1\
Jubjub	1\
Long	1\
O	1\
One,	2\
So	1\
The	4\
Tumtum	1\
and	7\
arms,	1\
as	2\
awhile	1\
back.	1\
beamish	1\
bird,	1\
bite,	1\
blade	1\
borogoves,	2\
boy!	1\
brillig,	2\
burbled	1\
by	1\
came!	1\
catch!	1\
chortled	1\
claws	1\
day!	1\
dead,	1\
eyes	1\
flame,	1\
foe	1\
frabjous	1\
frumious	1\
galumphing	1\
gimble	2\
gyre	2\
hand;	1\
hast	1\
he	3\
head	1\
his	2\
in	6\
it	2\
its	1\
jaws	1\
joy.	1\
left	1\
manxome	1\
mimsy	2\
mome	2\
my	3\
of	1\
outgrabe.	2\
raths	2\
rested	1\
shun	1\
slain	1\
slithy	2\
snicker-snack!	1\
son!	1\
sought\'97	1\
stood	1\
stood,	1\
sword	1\
that	2\
the	15\
thou	1\
thought	1\
thought.	1\
through	3\
time	1\
to	1\
took	1\
toves	2\
tree	1\
tulgey	1\
two!	2\
uffish	1\
vorpal	2\
wabe:	2\
went	2\
were	2\
whiffling	1\
with	2\
wood,	1\
\'92Twas	2\
\'93And	1\
\'93Beware	1
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\
}